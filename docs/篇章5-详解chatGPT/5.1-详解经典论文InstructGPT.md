# 详解经典论文InstructGPT

[InstructGPT经典论文链接](https://arxiv.org/pdf/2203.02155)
[论文中文版本](http://www.liaolinchun.com/archives/instructgpt-translate)
[文章讲解](https://huggingface.co/blog/zh/rlhf)
[视频讲解](https://www.bilibili.com/video/BV1fA411Z772/?spm_id_from=pageDriver&vd_source=35175d2a7dcd61ae778163f55d8e1297)

  ChatGPT四阶段
  预训练
  有监督微调
  奖励建模
  强化学习


  # 大模型alignment问题
  InstructGPT和后面的ChatGPT，都是OpenAI在大模型alignment问题上的研究成果。
  就是说，模型的输出，跟我们期待的，可能有所不一致。这个跟人类的需求的对齐问题，就是所谓的alignment问题。
  李宏毅老师的视频[Chat GPT是怎麼煉成的 - GPT 社會化的過程](https://www.youtube.com/watch?v=e0aKI2GGZNg)中把对大模型的跟人类需求一致性的改善过程，称为大模型的“社会化”过程，十分的形象。
  大模型在预训练过程中见识了各种各样的数据，因此针对一个prompt会输出什么东西，也可能是多种多样的，
  但是预训练数据中出现的数据模式，不代表都是人类在使用模型时希望看到的模式，因此需要一个社会化的过程，来规范模型的“言行举止”。

  ![大模型alignment问题](./pictures/001.png) 

  图：大模型alignment问题

  e.g. 给一个输入：“ACL会议的主题是什么”
  
  希望模型回答答案:上图中蓝色机器人的回答。
  
  实际模型可能个回答：图中红色机器人的输出。
  
  为什么呢？因为无论是“一个问题后面接一个回答”，还是“一个问题后面接另一个问题”，都是训练语料中可能经常出现的模式，因此，你让模型根据一个问题来续写，那无论是续写问题的答案，还是续写更多的问题，对于模型来说都是合理的。所以这就是问题所在。
  
  那么怎样让预训练的模型，在输出时符合人类的期待？

   #  RLHF 技术
   OpenAI 推出的 ChatGPT 对话模型掀起了新的 AI 热潮，它面对多种多样的问题对答如流，似乎已经打破了机器和人的边界。这一工作的背后是大型语言模型 (Large Language Model，LLM) 生成领域的新训练范式：RLHF (Reinforcement Learning from Human Feedback) ，即以强化学习方式依据人类反馈优化语言模型。

   RLHF 是一项涉及多个模型和不同训练阶段的复杂概念，这里我们按4个步骤分解：

* 预训练一个语言模型 (LM) ；
* 有监督微调LM获得SFT模型；
* 聚合问答数据并训练一个奖励模型 (Reward Model，RM) ；
* 用强化学习 (RL) 方式微调 SFT。

下面我们用OpenAI第一个流行的 RLHF 模型 InstructGPT 来进行讲解。
InstructGPT 中使用了较小版本的 GPT-3; Anthropic 使用了 1000 万 ～ 520 亿参数的 Transformer 模型进行训练；DeepMind 使用了自家的 2800 亿参数模型 Gopher

实验结果：InstructGPT使用1.3B参数的模型对比GPT-3的1750B参数模型能取得更优的性能（即能生成更优，更有用，更安全，更无害的内容，调戏过ChatGPT的人应该更有体会）。

  # InstructGPT

  ![instructGPT训练过程](./pictures/002.png) 

  图：instructGPT训练过程

  ## step 1. 预训练
  初始模型为V0，对于instructGPT来说，也就是GPT-3。GPT-3就是一个预训练模型。

### 工作原理：
在预训练阶段，模型使用来自网络的大规模数据集（如书籍、文章、百科、论坛内容）进行自回归语言建模。
自回归模型意味着：给定前面所有词，模型预测下一个词。这使模型能够学会语言的语法结构和常识性知识。

### GPT3预训练数据集：
来自网络，数据主要来源于开放的互联网语料库，如Common Crawl、BooksCorpus等，过滤掉明显低质量或有害的内容。这个数据集属于来自NLP数据集、网络爬虫和OpenAI从其他公司拿来的。可参考GPT-3（Language Models are Few-Shot Learners）的工作。

### 公式推导：
预训练目标最大化生成句子的概率：

$\mathcal{L}_{\text{pre}} = - \sum_{t=1}^{T} \log P_\theta (x_t \mid x_{<t})$

* $x_t$表示时间步t上的单词或标记（token）。
* $x_{<t}$表示在时间步t前的所有词汇。
* $\theta$是模型参数。
  
通过梯度下降优化该损失函数，模型可以从大量文本中学习语言模式和知识。

## step 2.有监督微调（Supervised Fine-Tuning, SFT）

### 工作原理：
* 微调阶段，模型在人工标注的高质量数据集上进行训练，进一步调整参数以匹配特定的任务需求。
* 标注过程：标注员根据特定任务（如“解释月球登陆给6岁儿童”）编写输出。这些高质量的label data作为数据集，帮助模型生成更符合任务需求的回答。
  
### 训练SFT模型的数据集（数据量：13k）：
* 预训练模型指令数据库中的API生产的prompt + 人工写的针对于prompt的answer

数据由人工标注员生成，包括与各种指令相关的输入和期望的输出。这里我们随机从指令库中采样出一些指令prompt，并人工给出答案。
最后，将这些指令和答案作为新的数据集进一步训练GPT-3模型，这里是由人工给出的答案作为标签，所以是有监督学习。我们将这一阶段完成后的模型称为SFT（supervised fine-tuning )。
  
### 公式推导：
该阶段的损失函数是标准的交叉熵损失，用于监督模型学习标注员给定的目标输出：
$\mathcal{L}_{\text{SFT}} = -\sum_{(x, y) \in D} \log P_\theta (y \mid x)$

* x 是输入指令，y 是标注员的目标输出。
* D 是训练数据集的集合。
* $\theta$是模型参数。
这个过程确保模型能够生成更贴近人类期望的响应。


## step 3. 奖励建模（Reward Modeling, RM）
### 工作原理：
* 目的：利用人类的偏好来训练模型，让模型能够生成更符合用户需求的回答。
* 原理：给定一个prompt（输入指令）后，模型会生成多个候选输出。标注员对这些输出进行质量比较，人工给出一个从最佳到最差的排序。模型需要学习标注员的偏好，并通过优化奖励模型的参数，使其能够评估和排序输出的优劣。
  
### 训练RM的排名数据集（数据量：33k）：
* Prompt = 模型指令数据库中的API生产的prompt + 人工写的prompt中获得指令
* Answer = SFT模型(InstructGPT中间形态)对上述promot进行回答
* 最后这些回答进行排名得到排名数据集（prompt，Rank）。
* 该数据集会用于训练得到一个RM模型。

### 训练过程：模型如何学习人类偏好
1. 模型输出与排序：
    * 给定一个prompt x，模型会生成 K 个不同的输出。
    * 每对输出之间都可以进行两两比较，因此总共会有 $\binom{K}{2} = \frac{K(K-1)}{2}$次比较。
2. 标注员给出排序：
    * 对于输出$y_w$和$y_l$，如果$y_w$的质量优于$y_l$,标注员会给出一个偏好关系： $y_w \succ y_l$。
    * 模型的目标是训练一个奖励模型 $r_\theta(x, y)$来尽量匹配这些偏好关系。

   3. 损失函数的推导与公式说明

  损失函数：
$\text{loss}(\theta) = - \frac{1}{\binom{K}{2}} \mathbb{E}_{(x, y_w, y_l) \sim D} \left[ \log \sigma \left( r_\theta(x, y_w) - r_\theta(x, y_l) \right) \right]$

符号说明：
    * x：输入的prompt。
    * $y_w$：标注员认为更优的回答（winner）。
    * $y_l$：标注员认为较差的回答（loser）。
    * $r_\theta(x, y)$：奖励模型对输入x和输出 y的评分。
    * $\sigma(\cdot)$：表示逻辑函数 $\sigma(z) = \frac{1}{1 + e^{-z}}$. 

解释：
    * 奖励模型的目标是最大化标注员偏好关系的概率，即希望 $r_\theta(x, y_w) > r_\theta(x, y_l)$。
    * 损失函数通过比较 $r_\theta(x, y_w)$和$r_\theta(x, y_l)$的差值，鼓励模型输出的评分符合标注员的排序。训练时需要最大化这个差值，即最小化损失函数。

注意：
 * 单个epoch训练：由于数据是从人类标注中获得的，两两比较耗时且复杂，因此模型通常只训练一个epoch。

 * 模型架构：奖励模型的架构基于SFT微调后的语言模型，但去除了最后的unembedding层，只保留了生成奖励分数的部分。


## step 4. 强化学习（Reinforcement Learning, RL）
这一步使用近端策略优化（Proximal Policy Optimization, PPO）继续微调上面的SFT模型，使SFT模型输出更符合奖励模型RM的评估和人类的期望。

1. 强化学习（RL）的目的和工作原理
在微调阶段，通过强化学习RL优化模型，使其基于奖励模型RM的反馈生成高质量输出：

* 输入：给定一个prompt x（例如“写一段关于青蛙的故事”）。
* 目标：通过强化学习调整模型，使其输出的质量得到奖励模型$r_\theta(x, y)$的高评分。
* 挑战：RL模型和之前的SFT模型之间的输出可能存在不一致，因此使用PPO算法来控制更新的幅度，保证稳定性。

2. PPO数据集（数据量：31k）
Prompt = 指令数据库的API获得的prompt，
answer = SFT模型根据prompt得到answer
ranking：RM模型给出这些answer的分数。
最后这个数据集由多个三元组（prompt，answer，RM给出的分数ranking）组成，用于进一步采用PPO算法微调SFT模型，得到最终的InstructGPT

3. 公式解析与推导

目标函数（Objective Function）：
$\text{objective}(\phi) = \mathbb{E}_{(x, y) \sim D_{\pi_\phi^{RL}}} \left[ r_\theta(x, y) - \beta \log \left( \frac{\pi_\phi^{RL}(y \mid x)}{\pi^{SFT}(y \mid x)} \right) \right] + \gamma \mathbb{E}_{x \sim D_{\text{pretrain}}} \left[ \log \pi_\phi^{RL}(x) \right]$

符号说明：
* x：输入的prompt。
* y：模型的输出（answer）。
* $r_\theta(x, y)$：奖励模型对给定输入 x和输出 y的评分, 即我们要最大化这个奖励，越大越好。
* $\pi_\phi^{RL}(y \mid x)$：RL模型在给定 x后输出 y的概率分布。
* $\pi^{SFT}(y \mid x)$：SFT模型在给定 x后输出 y的概率分布。
* $\beta$：用于平衡RL模型和SFT模型输出分布的超参数。
* $\gamma$：对预训练数据的正则化系数。

公式解释：
1. 奖励优化项 $r_\theta(x, y)$：
    * 强化学习的目标是最大化奖励模型RM的输出分数，即模型希望生成高评分的输出。这个项鼓励模型不断调整输出，使其在奖励模型中获得更高评价。
2. 策略正则化项 $\log \left( \frac{\pi_\phi^{RL}(y \mid x)}{\pi^{SFT}(y \mid x)} \right)$：
    * 该项度量RL模型和SFT模型输出分布的差异。如果RL模型和SFT模型的输出差异过大，损失会增加。因此，这个项起到了限制策略更新幅度的作用，确保RL模型不会过度偏离SFT模型的输出。
3. 预训练数据正则化项 $\log \pi_\phi^{RL}(x)$：
    * 该项用于确保模型在强化学习过程中不会忘记预训练阶段学到的知识，维持生成文本的多样性。

4. PPO的作用与优化过程
**近端策略优化（PPO）**是强化学习的一种算法，能够有效解决策略更新不稳定的问题：
* PPO通过限制新旧策略之间的变化幅度，使每次策略更新更加稳定，避免模型出现剧烈的参数更新。
* 在这里，$\beta$控制RL模型与SFT模型分布的一致性，确保RL模型不会在更新过程中产生不合理的输出。

5. 总结与应用
* RL优化的核心：最大化奖励模型的得分，同时限制RL模型与SFT模型的输出分布差异。
* PPO的优势：通过控制策略更新的幅度，保持了训练的稳定性，提高了模型的生成质量。
* 最终效果：经过RL训练后的模型能够在真实任务中生成符合人类偏好的高质量输出。
这一阶段的RL过程与PPO结合，确保了模型在奖励驱动下的稳步优化，使InstructGPT生成的答案更加符合人类期望。


## 最终过程概览
* 预训练模型: 也就是预训练好的GPT-3。。
* 有监督微调模型SFT：先人工构造一批labeled data，不用数量很大，先让模型GPT-3有监督学一学，得到SFT模型。
* 奖励模型RM: 让SFT模型根据一堆prompt进行多个输出，看看效果咋地，然后让人对多个输出进行人工打分排序。这里得到的打分排序数据也是标注数据，我们可以拿来再训练一个打分模型，称为RM（reward-model）。RM的作用就是可以对一个<prompt,output>pair打分，评价这个output跟prompt搭不搭。
* PPO强化学习得到最终instructGPT:  接下来，我们继续训练SFT模型。给定一些prompt，得到输出之后，把prompt和output输入给RM，得到打分。然后借助强化学习的方法，来继续训练SFT模型，如此反复迭代，最终修炼得到最终的InstructGPT。  

  
  

